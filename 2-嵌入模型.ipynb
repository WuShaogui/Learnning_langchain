{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将经过分割后外挂知识，使用嵌入模型计算其嵌入向量，便于后续在低维查询语义相关性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载embedding模型\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "encode_kwargs = {\"normalize_embeddings\": False}\n",
    "model_kwargs = {\"device\": \"cuda:0\"}\n",
    "embeddings_model= HuggingFaceEmbeddings(\n",
    "    model_name='/mnt/wushaogui/huggingface/shibing624/text2vec-base-chinese/',\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain 中的基 Embeddings 类提供了两种方法：一种用于嵌入文档，另一种用于嵌入查询。前者 `.embed_documents`，接受多个文本作为输入，而后者 `.embed_query`，接受单个文本作为输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 768)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"Hi there!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"What's your name?\",\n",
    "        \"My friends call me World\",\n",
    "        \"Hello World!\"\n",
    "    ]\n",
    ")\n",
    "len(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768 [0.5945923924446106, -0.5597456097602844, -0.2421606183052063, 1.2432446479797363, 0.19673125445842743]\n"
     ]
    }
   ],
   "source": [
    "embedded_query =embeddings_model.embed_query(\"人生如戏...\")\n",
    "\n",
    "print(len(embedded_query),embedded_query[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：这里的embedding和词向量不是一个概念，或者说最终的embedding是由多个词向量计算得来\n",
    "\n",
    "在传统的序列到序列（seq2seq）模型中，编码器（Encoder）会将不定长的输入文本编码为一个固定长度的向量，这个过程通常通过循环神经网络（RNN），如长短期记忆网络（LSTM）或门控循环单元（GRU）来实现。这个固定长度的向量通常是一个隐藏状态（hidden state），它能够捕捉输入序列的上下文信息，并为解码器（Decoder）提供必要的信息来生成输出序列。这个向量的维度是设计时预先确定的，比如可以是512维、1024维等，每个维度代表一种潜在特征的权重或强度\n",
    "。\n",
    "\n",
    "在更现代的模型，如基于Transformer的模型中，每个词或标记（token）都会被编码为一个固定长度的向量，并且每个位置的词向量还会加上一个位置编码（Positional Encoding），以保持词序信息。位置编码通常是通过正弦和余弦函数的组合来生成的，这样可以保证不同位置的编码向量之间有一定的规律性，并且可以保持唯一性\n",
    "。\n",
    "\n",
    "对于一句话，虽然每个词被编码为固定长度的向量，但整个句子并不是简单地编码为一个矩阵。相反，句子中的每个词都有自己的向量表示，并且这些向量会通过模型进行处理，以生成最终的固定长度的向量表示。这个向量表示可以用于各种下游任务，如文本分类、情感分析等。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
